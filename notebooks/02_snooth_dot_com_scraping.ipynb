{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Snooth.com\n",
    "\n",
    "Scrape wine information, tasting notes, and wine labels from [Snooth.com](http://snooth.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "import multiprocess as mp\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import dill\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape catalog of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start the ssh tunnels\n",
    "! ../priv/scripts/ssh_tunnels.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncomputers = 16\n",
    "nthreads = 16\n",
    "\n",
    "port_nos = np.array([8081+x for x in range(ncomputers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to create the Selenium web driver\n",
    "\n",
    "def make_driver(port):\n",
    "    \n",
    "    service_args = ['--proxy=127.0.0.1:{}'.format(port), '--proxy-type=socks5']\n",
    "    \n",
    "    dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "    ua = UserAgent()\n",
    "    dcap.update({'phantomjs.page.settings.userAgent':ua.random})\n",
    "    \n",
    "    phantom_path = '/usr/bin/phantomjs'\n",
    "    \n",
    "    driver = webdriver.PhantomJS(phantom_path, \n",
    "                                   desired_capabilities=dcap,\n",
    "                                   service_args=service_args)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the url list\n",
    "base_url = 'http://www.snooth.com/wines/#action=search&hide_state=1&country=US&color[0]={}&entity=wine&store_front=0&search_page={}'\n",
    "\n",
    "# red wines\n",
    "scrape_urls = [base_url.format(0, pg) for pg in range(1,1001)]\n",
    "\n",
    "# white wines\n",
    "scrape_urls.extend([base_url.format(1, pg) for pg in range(1,1001)])\n",
    "\n",
    "# rose\n",
    "scrape_urls.extend([base_url.format(2, pg) for pg in range(1,468)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_urls = [x for x in enumerate(scrape_urls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color_dict = {'0':'red', '1':'white', '2':'rose'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def master_scrape_urls(args):\n",
    "    port = args[0]\n",
    "    scrape_list = args[1]\n",
    "    \n",
    "    driver = make_driver(port)\n",
    "\n",
    "    url_list = list()\n",
    "\n",
    "    for u in scrape_list:\n",
    "        url_no = u[0]\n",
    "        url = u[1]\n",
    "        \n",
    "        if url_no % 100 == 0:\n",
    "            print url_no\n",
    "            \n",
    "            if len(url_list) > 0:\n",
    "                with open('../priv/pkl/02_snooth_dot_com_url_{}.pkl'.format(url_no-100), 'w') as fh:\n",
    "                    dill.dump(url_list, fh)\n",
    "            url_list = list()\n",
    "            \n",
    "            \n",
    "        color_no = re.search(r\"\"\"color\\[0\\]=([0-2])&\"\"\", url).group(1)\n",
    "        color = color_dict[color_no]\n",
    "    \n",
    "        driver.get(url)\n",
    "        time.sleep(1.1)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        the_list = [x.find('a', href=True).get('href') \n",
    "                         for x in soup.find_all(attrs={'class':'wine-name'}) \n",
    "                         if x.find('a', href=True) is not None]\n",
    "        the_list = [(color, x, url_no) for x in the_list]\n",
    "        url_list.extend(the_list)\n",
    "        \n",
    "    with open('../priv/pkl/02_snooth_dot_com_url_{}.pkl'.format((url_no//100)*100), 'w') as fh:\n",
    "        dill.dump(url_list, fh)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(scrape_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the url list up for scraping\n",
    "split_urls = list()\n",
    "\n",
    "for i in range(12):\n",
    "    begin = i*200\n",
    "    if i != (12-1):\n",
    "        end = (i+1)*200\n",
    "        split_urls.append(scrape_urls[begin:end])\n",
    "    else:\n",
    "        split_urls.append(scrape_urls[begin:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = mp.Pool(processes=12)\n",
    "results = pool.map(master_scrape_urls, [x for x in zip(port_nos[:12], split_urls)])\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_data_list = glob('../pkl/02_snooth_dot_com_url_*.pkl')\n",
    "int_sorter = lambda x: int(re.search(r\"\"\"_([0-9]+)\\.pkl\"\"\", x).group(1))\n",
    "url_data_list = sorted(url_data_list, key=int_sorter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_urls = list()\n",
    "for fil in url_data_list:\n",
    "    with open(fil, 'r') as fh:\n",
    "        combined_urls.extend(dill.load(fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('red',\n",
       " 'http://www.snooth.com/wine/frescobaldi-castiglioni-chianti-2011-6/',\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_urls = [(x[1][0],x[1][1],x[0]) for x in enumerate(combined_urls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rose',\n",
       " 'http://www.snooth.com/wine/bernard-griffin-rose-of-sangiovese-2009/',\n",
       " 49411)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_urls[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49412"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../priv/pkl/02_snooth_dot_com_url_list.pkl','w') as fh:\n",
    "    dill.dump(combined_urls, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reload or load if the variable doesn't exist\n",
    "with open('../priv/pkl/02_snooth_dot_com_url_list.pkl','r') as fh:\n",
    "    combined_urls = dill.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then scrape the URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_data(args):\n",
    "    \n",
    "    port = args[0]\n",
    "    url_list = args[1]\n",
    "\n",
    "    driver = make_driver(port)\n",
    "    \n",
    "    req = requests.session()\n",
    "    req_proxy = {'http': \"socks5://127.0.0.1:{}\".format(port)}\n",
    "    \n",
    "    wine_df_list = list()\n",
    "    \n",
    "    for url in url_list:\n",
    "\n",
    "        color = url[0]\n",
    "        full_url = url[1]\n",
    "        url_no = url[2]    \n",
    "\n",
    "        if (url_no % 100) == 0:\n",
    "            print url_no\n",
    "\n",
    "            if len(wine_df_list) > 0:\n",
    "                wine_df = pd.concat(wine_df_list, axis=0).reset_index(drop=True)\n",
    "                wine_df.to_pickle('../priv/pkl/02_snooth_dot_com_data_{}.pkl'.format(url_no-100))\n",
    "\n",
    "            wine_df_list = list()\n",
    "            \n",
    "            \n",
    "        driver.get(full_url)\n",
    "        time.sleep(1.2)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        wine_block1 = soup.find(attrs={'class':'wpp2014-wine_block-info'})\n",
    "        wine_block2 = soup.find(attrs={'class':'wpp2014-reg_rat-region_vintage'})\n",
    "\n",
    "        # wine name and year\n",
    "        try:\n",
    "            wine = wine_block1.find(attrs={'id':'wine-name'}).text.strip()\n",
    "        except:\n",
    "            wine = ''\n",
    "            year = ''\n",
    "\n",
    "        try:\n",
    "            year = re.search(r\"\"\"((?:20|19)[0-9]{2})\"\"\", wine).group(1)\n",
    "            wine = wine.replace(year, '').strip()\n",
    "        except:\n",
    "            year = ''\n",
    "\n",
    "        # review\n",
    "        try:\n",
    "            review = wine_block1.find(attrs={'class':'winemakers-notes'}).text.replace(\"Winemaker's Notes:\", '').strip()\n",
    "        except:\n",
    "            review = review.strip()\n",
    "\n",
    "        # prices\n",
    "        try:\n",
    "            price_list = wine_block1.find(attrs={'class':'wpp2014-wine_block-sample_prices'}).find_all(attrs={'itemprop':'price'})\n",
    "        except:\n",
    "            price_list = np.NaN\n",
    "        else:\n",
    "            price_list = np.array([float(x.text) for x in price_list]).mean()\n",
    "\n",
    "\n",
    "\n",
    "        # region, winery, varietal\n",
    "        region = ''\n",
    "        winery = ''\n",
    "        varietal = ''\n",
    "\n",
    "        try:\n",
    "            data_list = [re.split(r\"\"\"\\s+\"\"\", x.text.replace(u'\\xbb', '').strip()) \n",
    "                         for x in wine_block2.find_all('div')]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            for l in data_list:\n",
    "                if 'region' in l[0].lower():\n",
    "                    region = ' '.join(l[1:])\n",
    "                if 'winery' in l[0].lower():\n",
    "                    winery = ' '.join(l[1:])\n",
    "                if 'varietal' in l[0].lower():\n",
    "                    varietal = ' '.join(l[1:])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # get the image\n",
    "        try:\n",
    "            image_url = soup.find(attrs={'id':'wine-image-top'}).get('src')\n",
    "\n",
    "            if len(image_url) > 0:\n",
    "                filext = os.path.splitext(image_url)[-1]\n",
    "                path = '../priv/images/snooth_dot_com_' + str(url_no) + filext\n",
    "                img = req.get(image_url, proxies=req_proxy)\n",
    "                time.sleep(1.2)\n",
    "                \n",
    "                # print image_url, url_no, path\n",
    "\n",
    "                if img.status_code == 200:\n",
    "                    with open(path, 'wb') as f:\n",
    "                        for chunk in img:\n",
    "                            f.write(chunk)\n",
    "\n",
    "        except:\n",
    "            image_url = ''\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({'wine':wine, 'year':year, \n",
    "                           'review':review,\n",
    "                           'region':region, 'winery':winery, 'varietal':varietal,\n",
    "                           'price':price_list,\n",
    "                           'color':color, 'url':full_url,\n",
    "                           'image_url':image_url, 'url_no':url_no}, index=pd.Index([0]))\n",
    "\n",
    "        wine_df_list.append(df)\n",
    "        \n",
    "        \n",
    "    wine_df = pd.concat(wine_df_list, axis=0).reset_index(drop=True)\n",
    "    wine_df.to_pickle('../priv/pkl/02_snooth_dot_com_data_{}.pkl'.format(str((url_no//100)*100)))\n",
    "    \n",
    "    return            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the completed data\n",
    "\n",
    "file_list = glob('../priv/pkl/02_snooth_dot_com_data_*.pkl')\n",
    "int_sorter = lambda x: int(re.search(r\"\"\"_([0-9]+)\\.pkl\"\"\", x).group(1))\n",
    "file_list = sorted(file_list, key=int_sorter)\n",
    "\n",
    "file_nums = np.array(map(int_sorter, file_list))\n",
    "\n",
    "all_nums = np.arange(0, 49311, 100)\n",
    "redo_nums = all_nums[np.invert(np.in1d(all_nums, file_nums))]\n",
    "\n",
    "redo_lims = np.array(list(zip(redo_nums, redo_nums+100)))\n",
    "if redo_lims[0,-1] == 49300:\n",
    "    redo_lims[-1,-1] = 49500\n",
    "\n",
    "bool_selector = [(((x[-1] >= redo_lims[:,0])&(x[-1] < redo_lims[:,1]))==True).any() for x in combined_urls]\n",
    "\n",
    "selected_nums = [x for (x,y) in zip(combined_urls, bool_selector) if y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2000,  2300,  7900,  8400, 29900])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redo_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_urls = list()\n",
    "\n",
    "nthreads = 5\n",
    "\n",
    "for i in range(nthreads):\n",
    "    begin = i*100\n",
    "    if i != (nthreads - 1):\n",
    "        end = (i+1)*100\n",
    "        split_urls.append(selected_nums[begin:end])\n",
    "    else:\n",
    "        split_urls.append(selected_nums[begin:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('white',\n",
       " 'http://www.snooth.com/wine/reichsrat-von-buhl-von-buhl-riesling-trocken-2012/',\n",
       " 29999)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_urls[4][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "7900\n",
      "29900\n",
      "8400\n",
      "2300\n"
     ]
    }
   ],
   "source": [
    "# Do the scrape\n",
    "pool = mp.Pool(processes=nthreads)\n",
    "results = pool.map(scrape_data, [x for x in zip(port_nos, split_urls)])\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! echo \"pushover 'scrape finished'\" | /bin/zsh"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
