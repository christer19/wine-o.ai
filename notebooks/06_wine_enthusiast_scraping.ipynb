{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape professional reviews from WineEnthusiast.com\n",
    "\n",
    "After some testing, it seemed that amateur reviews weren't clustering well because they are very inconsistent. So professional wine reviews were scraped from [WineEnthusiast.com](http://wineenthusiast.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from fake_useragent import UserAgent\n",
    "import multiprocess as mp\n",
    "\n",
    "import subprocess\n",
    "import dill\n",
    "import re\n",
    "import time\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "\n",
    "Wine Enthusiast is quick to block. So a set of functions have been created that scrape through multiple proxies and will do so in parallel. The proxies are created from a cluster of EC2 instances on AWS started with StarCluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to create the Selenium web driver\n",
    "\n",
    "def make_driver(port):\n",
    "    \n",
    "    service_args = ['--proxy=127.0.0.1:{}'.format(port), '--proxy-type=socks5']\n",
    "    \n",
    "    dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "    ua = UserAgent()\n",
    "    dcap.update({'phantomjs.page.settings.userAgent':ua.random})\n",
    "    \n",
    "    phantom_path = '/usr/bin/phantomjs'\n",
    "    \n",
    "    driver = webdriver.PhantomJS(phantom_path, \n",
    "                                   desired_capabilities=dcap,\n",
    "                                   service_args=service_args) \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a page with a listing of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A function to scrape all the contents of the review links on a given page with a list of reviews\n",
    "\n",
    "def scrape_list(url_no, driver):\n",
    "    base_url = 'http://www.winemag.com/?s=&drink_type=wine&page={}'\n",
    "    url = base_url.format(url_no)\n",
    "    \n",
    "    scrape_dict = dict()\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1.25)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'xml')\n",
    "        review_list = [x.get('href') for x in soup.find_all(attrs={'class':'review-listing'}, href=True)]\n",
    "        success = True\n",
    "    except:\n",
    "        scrape_dict[url_no] = np.NaN\n",
    "        success = False\n",
    "    \n",
    "    if success:\n",
    "        review_list = np.array(review_list)\n",
    "        np.random.shuffle(review_list)\n",
    "        for review in review_list:\n",
    "            review_result = scrape_review(review, driver)\n",
    "            time.sleep(np.random.rand()*1.5)\n",
    "            scrape_dict[(url_no, review)] = review_result\n",
    "    return scrape_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape each of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function to scrape a single review\n",
    "\n",
    "def scrape_review(url, driver):     \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(np.random.rand()*1.5+1.5)\n",
    "        html = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        success = True\n",
    "        \n",
    "    except:\n",
    "        success = False        \n",
    "        \n",
    "    if success:\n",
    "        # scrape the data\n",
    "        value_dict = dict()\n",
    "\n",
    "        value_dict['url'] = url[1]\n",
    "\n",
    "        try:\n",
    "            title = soup.find(attrs={'class':'article-title'}).text\n",
    "            value_dict['title'] = title\n",
    "        except:\n",
    "            value_dict['title'] = ''\n",
    "\n",
    "        try:\n",
    "            rating = soup.find(attrs={'id':'points'}).text\n",
    "            value_dict['rating'] = rating\n",
    "        except:\n",
    "            value_dict['rating'] = ''\n",
    "\n",
    "        try:\n",
    "            review = soup.find(attrs={'class':'description'}).text\n",
    "            value_dict['review'] = review\n",
    "        except:\n",
    "            value_dict['review'] = ''\n",
    "\n",
    "        try:\n",
    "            primary_info = soup.find(attrs={'class':'primary-info'})\n",
    "            primary_keys = [x.text.strip().lower() \n",
    "                            for x in primary_info.find_all(attrs={'class':'info-label medium-7 columns'})]\n",
    "            primary_values = [x.text.strip().encode('utf-8') \n",
    "                              for x in primary_info.find_all(attrs={'class':'info medium-9 columns'})]\n",
    "\n",
    "            try:\n",
    "                price = re.search(r\"\"\"\\$([0-9]+)\"\"\", primary_values[0]).group(1)\n",
    "                primary_values[0] = price\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            value_dict.update(dict(zip(primary_keys, primary_values)))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            secondary_info = soup.find(attrs={'class':'secondary-info'})\n",
    "\n",
    "            secondary_keys = [x.text.strip().lower().replace(' ', '_') \n",
    "                              for x in secondary_info.find_all(attrs={'class':'info-label small-7 columns'})]\n",
    "            secondary_values = [x.text.strip() \n",
    "                                for x in secondary_info.find_all(attrs={'class':'info small-9 columns'})]\n",
    "\n",
    "            value_dict.update(dict(zip(secondary_keys, secondary_values)))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if success:\n",
    "        return pd.Series(value_dict)\n",
    "    else:\n",
    "        return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master function to start PhantomJS instance and scrape URL list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A master scraping function for a port and list of URLS\n",
    "\n",
    "def master_scrape(args):\n",
    "    \n",
    "    port_nos = args[1]\n",
    "    url_nos = args[0]\n",
    "    nsplit = len(port_nos)\n",
    "\n",
    "    for url_split,port_ in zip(np.array_split(url_nos, nsplit), \n",
    "                               np.array_split(port_nos, nsplit)):\n",
    "\n",
    "        port = port_[0]\n",
    "        np.random.shuffle(url_split)\n",
    "        \n",
    "        driver = make_driver(port)\n",
    "\n",
    "\n",
    "        for no in url_split:\n",
    "            time.sleep(np.random.rand()*5+5)\n",
    "            try:\n",
    "                scrape_dict = scrape_list(no, driver)\n",
    "                with open('../priv/pkl/06_wine_enthusiast_dot_com_data_{}.pkl'.format(no), 'w') as fh:\n",
    "                    dill.dump(scrape_dict, fh)\n",
    "                print no\n",
    "            except:\n",
    "                print 'ERROR: ' + no\n",
    "                pass\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start the ssh tunnels\n",
    "! ../priv/scripts/ssh_tunnels.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do all the scraping\n",
    "# Note that I ended up doing this in two parts (pages 1-3000 and pages 3001-6529)\n",
    "# This was so that I could terminate the cluster in the middle and create a new\n",
    "# one, resulting in different IP addresses to proxy through for the scrape\n",
    "\n",
    "nthreads = 16\n",
    "ncomputers = 16\n",
    "\n",
    "# url_nos = np.arange(1, 3001)\n",
    "url_nos = np.arange(3001, 6530)\n",
    "\n",
    "np.random.shuffle(url_nos)\n",
    "\n",
    "port_nos = np.array([8081+x for x in range(ncomputers)])\n",
    "\n",
    "pool = mp.Pool(processes=nthreads)\n",
    "results = pool.map(master_scrape, [x for x in zip(np.array_split(url_nos, nthreads), \n",
    "                                                  np.array_split(port_nos, nthreads))])\n",
    "pool.close()\n",
    "\n",
    "! echo \"pushover 'scrape finished'\" | /bin/zsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try again with pages that seem to have been skipped\n",
    "\n",
    "# Get the missing pages\n",
    "from glob import glob\n",
    "file_list = glob('../priv/pkl/06_wine_enthusiast_dot_com_data_*.pkl')\n",
    "int_sorter = lambda x: int(re.search(r\"\"\"06_wine_enthusiast_dot_com_data_(.+).pkl\"\"\", x).group(1))\n",
    "file_list = sorted(file_list, key=int_sorter)\n",
    "\n",
    "full_list = np.arange(1,6530)\n",
    "num_list = np.array([int_sorter(x) for x in file_list])\n",
    "\n",
    "mask = np.invert(np.in1d(full_list, num_list))\n",
    "url_nos = full_list[mask]\n",
    "###\n",
    "\n",
    "np.random.shuffle(url_nos)\n",
    "\n",
    "pool = mp.Pool(processes=nthreads)\n",
    "results = pool.map(master_scrape, [x for x in zip(np.array_split(url_nos, nthreads), \n",
    "                                                  np.array_split(port_nos, nthreads))])\n",
    "pool.close()\n",
    "\n",
    "! echo \"pushover 'scrape finished'\" | /bin/zsh"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
