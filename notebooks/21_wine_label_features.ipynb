{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data import\n",
    "\n",
    "Separate out the large (test) images from the main library for faster development work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4117, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_images = pd.read_pickle('../pkl/20_wine_label_analysis_large_labels.pkl')\n",
    "large_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>image_name</th>\n",
       "      <th>width</th>\n",
       "      <th>area</th>\n",
       "      <th>basename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1200</td>\n",
       "      <td>../images/snooth_dot_com_151.png</td>\n",
       "      <td>874</td>\n",
       "      <td>1048800</td>\n",
       "      <td>snooth_dot_com_151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>883</td>\n",
       "      <td>../images/snooth_dot_com_177.jpeg</td>\n",
       "      <td>1466</td>\n",
       "      <td>1294478</td>\n",
       "      <td>snooth_dot_com_177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>897</td>\n",
       "      <td>../images/snooth_dot_com_228.jpeg</td>\n",
       "      <td>1254</td>\n",
       "      <td>1124838</td>\n",
       "      <td>snooth_dot_com_228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1123</td>\n",
       "      <td>../images/snooth_dot_com_284.jpeg</td>\n",
       "      <td>974</td>\n",
       "      <td>1093802</td>\n",
       "      <td>snooth_dot_com_284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1032</td>\n",
       "      <td>../images/snooth_dot_com_406.jpeg</td>\n",
       "      <td>1260</td>\n",
       "      <td>1300320</td>\n",
       "      <td>snooth_dot_com_406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     height                         image_name  width     area  \\\n",
       "150    1200   ../images/snooth_dot_com_151.png    874  1048800   \n",
       "176     883  ../images/snooth_dot_com_177.jpeg   1466  1294478   \n",
       "227     897  ../images/snooth_dot_com_228.jpeg   1254  1124838   \n",
       "283    1123  ../images/snooth_dot_com_284.jpeg    974  1093802   \n",
       "405    1032  ../images/snooth_dot_com_406.jpeg   1260  1300320   \n",
       "\n",
       "               basename  \n",
       "150  snooth_dot_com_151  \n",
       "176  snooth_dot_com_177  \n",
       "227  snooth_dot_com_228  \n",
       "283  snooth_dot_com_284  \n",
       "405  snooth_dot_com_406  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47247, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = pd.read_pickle('../pkl/20_wine_label_analysis_all_labels.pkl')\n",
    "mask = all_images['basename'].isin(large_images['basename']).pipe(np.invert)\n",
    "all_images = all_images.loc[mask]\n",
    "all_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RootSIFT Features\n",
    "\n",
    "Determine the RootSIFT features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RootSIFT(object):\n",
    "    # From http://www.pyimagesearch.com/2015/04/13/implementing-rootsift-in-python-and-opencv/\n",
    "    \n",
    "    def __init__(self):\n",
    "        # initialize the SIFT feature extractor\n",
    "        self.extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
    "\n",
    "    def compute(self, image, kps, eps=1e-7):\n",
    "        # compute SIFT descriptors\n",
    "        (kps, descs) = self.extractor.compute(image, kps)\n",
    "\n",
    "        # if there are no keypoints or descriptors, return an empty tuple\n",
    "        if len(kps) == 0:\n",
    "            return ([], None)\n",
    "\n",
    "        # apply the Hellinger kernel by first L1-normalizing and taking the\n",
    "        # square-root\n",
    "        descs /= (descs.sum(axis=1, keepdims=True) + eps)\n",
    "        descs = np.sqrt(descs)\n",
    "        #descs /= (np.linalg.norm(descs, axis=1, ord=2) + eps)\n",
    "\n",
    "        # return a tuple of the keypoints and descriptors\n",
    "        return (kps, descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_image(image, height=None, width=None):\n",
    "    \n",
    "    (orig_height, orig_width) = image.shape[:2]\n",
    "    orig_height = float(orig_height)\n",
    "    orig_width = float(orig_width)\n",
    "\n",
    "    if height is not None:\n",
    "        ratio = height / orig_height\n",
    "        dim = (int(orig_width * ratio), height)\n",
    "\n",
    "    elif width is not None:\n",
    "        ratio = width / orig_width\n",
    "        dim = (width, int(orig_height * ratio))\n",
    "\n",
    "    resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sift_features(image_path_df):\n",
    "    \n",
    "    #len_list = list()\n",
    "    sift = RootSIFT()\n",
    "    feat_detector = cv2.FeatureDetector_create('SURF') # test SIFT, SURF\n",
    "\n",
    "    #for in_path in image_path_df['image_name'].values:\n",
    "    for in_path in image_path_df:\n",
    "        try:\n",
    "            image = cv2.imread(in_path)\n",
    "            image = resize_image(image, width=320)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            kps = feat_detector.detect(image)\n",
    "            (kps, descriptors) = sift.compute(image, kps)\n",
    "            kp = np.array([x.pt for x in kps])\n",
    "\n",
    "            out_path = in_path.replace('images','sift').replace('jpeg','pkl').replace('jpg','pkl').replace('png','pkl')\n",
    "            out_array = np.hstack((kp, descriptors))\n",
    "            with open(out_path, 'w') as fh:\n",
    "                dill.dump(descriptors, fh)\n",
    "\n",
    "            out_path_df = out_path.replace('.pkl', '_df.pkl')\n",
    "            descriptors_df = pd.DataFrame(descriptors)\n",
    "            descriptors_df['kp0'] = kp[:,0]\n",
    "            descriptors_df['kp1'] = kp[:,1]\n",
    "            descriptors_df.to_pickle(out_path_df)\n",
    "        except:\n",
    "            print in_path\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nthreads = 48\n",
    "pool = mp.Pool(processes=nthreads)\n",
    "pool.map(get_sift_features, np.array_split(large_images, nthreads))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nthreads = 48\n",
    "pool = mp.Pool(processes=nthreads)\n",
    "pool.map(get_sift_features, np.array_split(all_images, nthreads))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up additional missing data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47247 51019\n",
      "47247\n",
      "345\n"
     ]
    }
   ],
   "source": [
    "existing_pkl = glob('../sift/*.pkl')\n",
    "existing_pkl = np.array([x.replace('_df','') for x in existing_pkl if '_df.' in x])\n",
    "output_pkl = all_images.basename.apply(lambda x: '../sift/' + x + '.pkl').values\n",
    "\n",
    "print len(output_pkl), len(existing_pkl)\n",
    "\n",
    "mask = np.invert(np.in1d(output_pkl, existing_pkl))\n",
    "print(len(mask))\n",
    "\n",
    "missing = output_pkl[mask]\n",
    "missing = np.array([x.replace('sift','images').replace('pkl','*') for x in missing])\n",
    "print(len(missing))\n",
    "\n",
    "missing_images = [glob(x)[0] for x in missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../images/snooth_dot_com_21815.jpeg\n"
     ]
    }
   ],
   "source": [
    "nthreads = 16\n",
    "pool = mp.Pool(processes=nthreads)\n",
    "pool.map(get_sift_features, np.array_split(np.array(missing_images), nthreads))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! echo \"pushover 'sift finished'\" | /usr/bin/zsh3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create HDF5 data storage of feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# st = pd.HDFStore('../data/features_large_images.h5', mode='w')\n",
    "# combined_images = large_images\n",
    "\n",
    "st = pd.HDFStore('../data/features.h5', mode='w')\n",
    "combined_images = pd.concat([large_images, all_images], axis=0)\n",
    "\n",
    "beg = 0\n",
    "\n",
    "basename_list = list()\n",
    "index_list = list()\n",
    "imagepath_list = list()\n",
    "\n",
    "for row,dat in combined_images.iterrows():\n",
    "    basename = dat.basename\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_pickle('../sift/' + basename + '_df.pkl')\n",
    "        end = beg+df.shape[0]\n",
    "\n",
    "        kp = df[['kp0','kp1']]\n",
    "        df = df.drop(['kp0','kp1'], axis=1)\n",
    "\n",
    "        index = pd.Index(np.arange(beg, end))\n",
    "        df = df.set_index(index)\n",
    "        kp = kp.set_index(index)\n",
    "\n",
    "        st.append('features', df, index=False)\n",
    "        st.append('keypoints', kp, index=False)\n",
    "\n",
    "        basename_list.append(basename)\n",
    "        index_list.append([beg, end])\n",
    "        imagepath_list.append(dat.image_name)\n",
    "\n",
    "        beg = end\n",
    "    except:\n",
    "        print basename\n",
    "\n",
    "st.append('basename', pd.Series(basename_list))\n",
    "st.append('image_path', pd.Series(imagepath_list))\n",
    "st.append('index', pd.DataFrame(np.array(index_list), columns=['beg','end']))\n",
    "\n",
    "st.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! echo \"pushover 'hdf5 feature database creation finished'\" | /usr/bin/zsh"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
