{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill\n",
    "\n",
    "import cv2\n",
    "from image_features import feature_detect_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data import\n",
    "\n",
    "Separate out the large (test) images from the main library for faster development work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4117, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_images = pd.read_pickle('../priv/pkl/20_wine_label_analysis_large_labels.pkl')\n",
    "large_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47247, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = pd.read_pickle('../priv/pkl/20_wine_label_analysis_all_labels.pkl')\n",
    "mask = all_images['basename'].isin(large_images['basename']).pipe(np.invert)\n",
    "all_images = all_images.loc[mask]\n",
    "all_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get image features\n",
    "\n",
    "Determine features using SIFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sift_features(image_path_df):\n",
    "    \n",
    "    #len_list = list()\n",
    "    feat_detector = cv2.FeatureDetector_create('SIFT') # test SIFT, SURF\n",
    "    feat_extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
    "\n",
    "    for in_path in image_path_df['image_name'].values:\n",
    "    #for in_path in image_path_df:\n",
    "        try:\n",
    "            kps, descs = feature_detect_extract(in_path, feat_detector, feat_extractor)\n",
    "            kp = np.array([x.pt for x in kps])\n",
    "\n",
    "            out_path = in_path.replace('images','sift').replace('jpeg','pkl').replace('jpg','pkl').replace('png','pkl')\n",
    "            out_array = np.hstack((kp, descs))\n",
    "            with open(out_path, 'w') as fh:\n",
    "                dill.dump(descs, fh)\n",
    "\n",
    "            out_path_df = out_path.replace('.pkl', '_df.pkl')\n",
    "            descriptors_df = pd.DataFrame(descs)\n",
    "            descriptors_df['kp0'] = kp[:,0]\n",
    "            descriptors_df['kp1'] = kp[:,1]\n",
    "            descriptors_df.to_pickle(out_path_df)\n",
    "        except:\n",
    "            print in_path\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nthreads = 48\n",
    "pool = mp.Pool(processes=nthreads)\n",
    "pool.map(get_sift_features, np.array_split(large_images, nthreads))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nthreads = 48\n",
    "pool = mp.Pool(processes=nthreads)\n",
    "pool.map(get_sift_features, np.array_split(all_images, nthreads))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up additional missing data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47247 51019\n",
      "47247\n",
      "345\n"
     ]
    }
   ],
   "source": [
    "existing_pkl = glob('../priv/sift/*.pkl')\n",
    "existing_pkl = np.array([x.replace('_df','') for x in existing_pkl if '_df.' in x])\n",
    "output_pkl = all_images.basename.apply(lambda x: '../sift/' + x + '.pkl').values\n",
    "\n",
    "print len(output_pkl), len(existing_pkl)\n",
    "\n",
    "mask = np.invert(np.in1d(output_pkl, existing_pkl))\n",
    "print(len(mask))\n",
    "\n",
    "missing = output_pkl[mask]\n",
    "missing = np.array([x.replace('sift','images').replace('pkl','*') for x in missing])\n",
    "print(len(missing))\n",
    "\n",
    "missing_images = [glob(x)[0] for x in missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../images/snooth_dot_com_21815.jpeg\n"
     ]
    }
   ],
   "source": [
    "nthreads = 16\n",
    "pool = mp.Pool(processes=nthreads)\n",
    "pool.map(get_sift_features, np.array_split(np.array(missing_images), nthreads))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! echo \"pushover 'sift finished'\" | /usr/bin/zsh3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create HDF5 data storage of feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# st = pd.HDFStore('../priv/data/features_large_images.h5', mode='w')\n",
    "# combined_images = large_images\n",
    "\n",
    "st = pd.HDFStore('../priv/data/features.h5', mode='w')\n",
    "combined_images = pd.concat([large_images, all_images], axis=0)\n",
    "\n",
    "beg = 0\n",
    "\n",
    "basename_list = list()\n",
    "index_list = list()\n",
    "imagepath_list = list()\n",
    "\n",
    "for row,dat in combined_images.iterrows():\n",
    "    basename = dat.basename\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_pickle('../priv/sift/' + basename + '_df.pkl')\n",
    "        end = beg+df.shape[0]\n",
    "\n",
    "        kp = df[['kp0','kp1']]\n",
    "        df = df.drop(['kp0','kp1'], axis=1)\n",
    "\n",
    "        index = pd.Index(np.arange(beg, end))\n",
    "        df = df.set_index(index)\n",
    "        kp = kp.set_index(index)\n",
    "\n",
    "        st.append('features', df, index=False)\n",
    "        st.append('keypoints', kp, index=False)\n",
    "\n",
    "        basename_list.append(basename)\n",
    "        index_list.append([beg, end])\n",
    "        imagepath_list.append(dat.image_name)\n",
    "\n",
    "        beg = end\n",
    "    except:\n",
    "        print basename\n",
    "\n",
    "st.append('basename', pd.Series(basename_list))\n",
    "st.append('image_path', pd.Series(imagepath_list))\n",
    "st.append('index', pd.DataFrame(np.array(index_list), columns=['beg','end']))\n",
    "\n",
    "st.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! echo \"pushover 'hdf5 feature database creation finished'\" | /usr/bin/zsh"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:scienv2_oldcv]",
   "language": "python",
   "name": "conda-env-scienv2_oldcv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
