{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill\n",
    "import os\n",
    "import warnings\n",
    "import h5py\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import pairwise\n",
    "#from scipy.sparse import csr_matrix, vstack\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with KMeans to form a codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4117, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data set of the higher resolution images\n",
    "large_images = pd.read_pickle('../pkl/20_wine_label_analysis_large_labels.pkl')\n",
    "large_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47247, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All remaining images\n",
    "all_images = pd.read_pickle('../pkl/20_wine_label_analysis_all_labels.pkl')\n",
    "mask = all_images['basename'].isin(large_images['basename']).pipe(np.invert)\n",
    "all_images = all_images.loc[mask]\n",
    "all_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in SIFT keypoints and perform k-means analysis to create codebook.\n",
    "\n",
    "## ** CURRENTLY ONLY USING ~10% OF DATA SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 51363\n",
      "Total features: 40120404\n",
      "Maximum index: 1666076\n"
     ]
    }
   ],
   "source": [
    "is_trial = True\n",
    "\n",
    "st = pd.HDFStore('../data/features.h5', 'r')\n",
    "\n",
    "mask = st['basename'].isin(large_images.basename)\n",
    "\n",
    "print('Total images: {}'.format(st['basename'].shape[0]))\n",
    "print('Total features: {}'.format(st['index']['end'].max()))\n",
    "\n",
    "if is_trial:\n",
    "    max_index = st['index'].loc[mask,'end'].max()\n",
    "else:\n",
    "    max_index = st['index']['end'].max()\n",
    "    \n",
    "print('Maximum index: {}'.format(max_index))\n",
    "\n",
    "st.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mini_batch_kmeans(data_path, out_path, max_index, n_clusters, frac_points=0.5):\n",
    "    \n",
    "    print n_clusters\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    \n",
    "    # Select randomized indexes for data and read it in\n",
    "    st = pd.HDFStore(data_path, 'r')\n",
    "    n_points = int(frac_points * max_index)\n",
    "    indexes = np.random.choice(np.arange(max_index), n_points, replace=False)\n",
    "    data = st['features'].loc[indexes].values\n",
    "    \n",
    "    \n",
    "    model = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, init_size=3*n_clusters)\n",
    "    model.fit(data)\n",
    "    \n",
    "    st.close()\n",
    "    \n",
    "    # Write the resulting model clusters out to a file\n",
    "    st = h5py.File(out_path, 'a')\n",
    "    if str(n_clusters) in st.keys():\n",
    "        st.pop(str(n_clusters))\n",
    "        \n",
    "    mod = st.create_dataset(str(n_clusters), model.cluster_centers_.shape)\n",
    "    mod[:,:] = model.cluster_centers_\n",
    "    \n",
    "    st.close()\n",
    "    \n",
    "    with open('../models/minibatch_kmeans_clusters_{}.pkl'.format(n_clusters),'wb') as fh:\n",
    "        dill.dump(model.cluster_centers_, fh)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "nclusters = [1500, 1536, 2000, 2500, 3000, 5000]\n",
    "\n",
    "for cluster in nclusters:\n",
    "    mini_batch_kmeans('../data/features.h5', '../data/kmeans.h5', max_index, cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! echo \"pushover 'kmeans clustering finished'\" | /usr/bin/zsh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
